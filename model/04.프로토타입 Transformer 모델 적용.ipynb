{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b511b0a3-8808-4822-b644-65b313ae3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a647184c-3d8a-433e-9bbc-22dcf3750992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./prototype_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9377faf-a7d8-4ae1-bb7c-ca51964916a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일시</th>\n",
       "      <th>PM-10</th>\n",
       "      <th>('풍속', 98)</th>\n",
       "      <th>('풍속', 99)</th>\n",
       "      <th>('풍속', 108)</th>\n",
       "      <th>('풍속', 112)</th>\n",
       "      <th>('풍속', 119)</th>\n",
       "      <th>('풍속', 133)</th>\n",
       "      <th>('풍속', 201)</th>\n",
       "      <th>('강수량', 98)</th>\n",
       "      <th>...</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>hour_00</th>\n",
       "      <th>hour_03</th>\n",
       "      <th>hour_06</th>\n",
       "      <th>hour_09</th>\n",
       "      <th>hour_12</th>\n",
       "      <th>hour_15</th>\n",
       "      <th>hour_18</th>\n",
       "      <th>hour_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03 03:00:00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.001528</td>\n",
       "      <td>-1.276354</td>\n",
       "      <td>-0.794339</td>\n",
       "      <td>0.077853</td>\n",
       "      <td>-0.865018</td>\n",
       "      <td>-0.777845</td>\n",
       "      <td>-1.041770</td>\n",
       "      <td>-0.194231</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-03 06:00:00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.001528</td>\n",
       "      <td>-1.178118</td>\n",
       "      <td>-0.692980</td>\n",
       "      <td>-0.654746</td>\n",
       "      <td>-1.128880</td>\n",
       "      <td>-0.777845</td>\n",
       "      <td>-1.209365</td>\n",
       "      <td>-0.194231</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03 09:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.902350</td>\n",
       "      <td>-1.276354</td>\n",
       "      <td>-0.895697</td>\n",
       "      <td>-1.533866</td>\n",
       "      <td>-1.304788</td>\n",
       "      <td>-0.863775</td>\n",
       "      <td>-0.957973</td>\n",
       "      <td>-0.194231</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-03 12:00:00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.604817</td>\n",
       "      <td>-0.981647</td>\n",
       "      <td>-0.084830</td>\n",
       "      <td>-0.508227</td>\n",
       "      <td>-0.513201</td>\n",
       "      <td>-0.262265</td>\n",
       "      <td>-0.957973</td>\n",
       "      <td>-0.194231</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-03 15:00:00</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.585315</td>\n",
       "      <td>-0.294000</td>\n",
       "      <td>0.827396</td>\n",
       "      <td>0.297633</td>\n",
       "      <td>0.982019</td>\n",
       "      <td>1.198545</td>\n",
       "      <td>0.466585</td>\n",
       "      <td>-0.194231</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 613 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    일시  PM-10  ('풍속', 98)  ('풍속', 99)  ('풍속', 108)  \\\n",
       "0  2023-01-03 03:00:00   27.0   -1.001528   -1.276354    -0.794339   \n",
       "1  2023-01-03 06:00:00   29.0   -1.001528   -1.178118    -0.692980   \n",
       "2  2023-01-03 09:00:00   30.0   -0.902350   -1.276354    -0.895697   \n",
       "3  2023-01-03 12:00:00   41.0   -0.604817   -0.981647    -0.084830   \n",
       "4  2023-01-03 15:00:00   31.0    0.585315   -0.294000     0.827396   \n",
       "\n",
       "   ('풍속', 112)  ('풍속', 119)  ('풍속', 133)  ('풍속', 201)  ('강수량', 98)  ...  \\\n",
       "0     0.077853    -0.865018    -0.777845    -1.041770    -0.194231  ...   \n",
       "1    -0.654746    -1.128880    -0.777845    -1.209365    -0.194231  ...   \n",
       "2    -1.533866    -1.304788    -0.863775    -0.957973    -0.194231  ...   \n",
       "3    -0.508227    -0.513201    -0.262265    -0.957973    -0.194231  ...   \n",
       "4     0.297633     0.982019     1.198545     0.466585    -0.194231  ...   \n",
       "\n",
       "   month_11  month_12  hour_00  hour_03  hour_06  hour_09  hour_12  hour_15  \\\n",
       "0     False     False    False     True    False    False    False    False   \n",
       "1     False     False    False    False     True    False    False    False   \n",
       "2     False     False    False    False    False     True    False    False   \n",
       "3     False     False    False    False    False    False     True    False   \n",
       "4     False     False    False    False    False    False    False     True   \n",
       "\n",
       "   hour_18  hour_21  \n",
       "0    False    False  \n",
       "1    False    False  \n",
       "2    False    False  \n",
       "3    False    False  \n",
       "4    False    False  \n",
       "\n",
       "[5 rows x 613 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097306da-3999-4738-aae7-2a45e41f2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.set_index(\"일시\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23015c54-1615-4a63-9ec2-257a78913aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df2.drop(\"PM-10\", axis = 1)\n",
    "y = df2[\"PM-10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95df6abf-c4e6-446b-a648-baf04b430ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 예측과 같은 작업에서는 단일 스택을 사용하여 입력 시퀀스를 처리\n",
    "def create_seq2seq_data(X, y, input_steps=52, output_steps=24): # input_steps는 우리가 예측하는 시간이고 output_steps는 예측할 시간(output_steps는 고정)\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - input_steps - output_steps):\n",
    "        Xs.append(X.iloc[i:(i + input_steps)].values)\n",
    "        ys.append(y.iloc[(i + input_steps):(i + input_steps + output_steps)].values)\n",
    "    return np.array(Xs).astype(np.float32), np.array(ys).astype(np.float32)\n",
    "\n",
    "input_steps = 52\n",
    "output_steps = 24\n",
    "X_seq2seq, y_seq2seq = create_seq2seq_data(x, y, input_steps, output_steps)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_seq2seq, y_seq2seq, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e592f3bd-8282-43ab-bccc-2cc640c11270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 모델 정의 - 트랜스포머 모델의 입력 데이터 형식은 (batch_size, sequence_length, num_features)\n",
    "class MultiHeadSelfAttention(Layer): # 입력으로 주어진 시퀀스를 각 헤드에 대해 독립적으로 어텐션을 수행하고, 그 결과를 결합하여 출력(병렬구조)\n",
    "    def __init__(self, embed_dim, num_heads=8): # embed_dim은 입력 임베딩 차원, num_heads는 어텐션을 수행할 헤드의 수\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0: # 입력 임베딩 차원이 헤드의 수로 나누어 떨어지지 않으면 ValueError가 발생\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads # 각 헤드의 프로젝션 차원은 embed_dim // num_heads\n",
    "        # query_dense, key_dense, value_dense 및 combine_heads는 각각 어텐션에 사용되는 dense 레이어\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    # 정보 손실 문제 해결\n",
    "    def attention(self, query, key, value): # attention 함수는 주어진 쿼리, 키 및 값에 대해 어텐션 가중치를 계산하고, 이를 사용하여 값을 가중합하여 출력을 생성\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size): # separate_heads 함수는 입력을 여러 헤드로 분리하는 메서드\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) # 입력 형태는 (batch_size, num_heads, seq_len, projection_dim) 형태로 변환\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs): # 입력에 대한 어텐션을 계산하는 메서드\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        # 입력에 대해 query_dense, key_dense, value_dense 레이어를 통과시키고, 그 결과를 각각 separate_heads() 메서드로 분리\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value) # 분리된 쿼리, 키 및 값에 대해 어텐션을 계산하고, 이를 결합\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) # 마지막으로, 결합된 어텐션 결과에 combine_heads 레이어를 적용하여 최종 출력을 생성\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadc463f-fda2-445a-8906-5923878d2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): # TransformerBlock은 여러 층으로 쌓여 전체 트랜스포머 모델을 형성하며, 입력 시퀀스의 특징을 추출하고 다음 층으로 전달\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): # embed_dim은 입력 임베딩 차원, num_heads는 멀티 헤드 셀프 어텐션에서 사용할 어텐션 헤드의 수\n",
    "        # ff_dim은 피드 포워드 신경망의 은닉층 크기, rate는 dropout의 비율\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) # att는 멀티 헤드 셀프 어텐션 레이어를 생성\n",
    "        self.ffn = Sequential( # ffn은 피드 포워드 신경망을 생성. 이는 두 개의 dense 레이어로 구성되어 있음. \n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),] # 첫 번째 레이어는 ReLU 활성화 함수를 사용하고, 두 번째 레이어는 선형 활성화 함수를 사용\n",
    "        )\n",
    "        # layernorm1, layernorm2는 레이어 정규화 레이어\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        # dropout1, 2 레이어도 설정\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False): # 입력을 받아와서 먼저 멀티 헤드 셀프 어텐션을 수행\n",
    "        # 어텐션 출력에 드롭아웃을 적용하고, 입력과 어텐션 출력을 더한 후 레이어 정규화를 수행\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        # 그 다음, 피드 포워드 신경망을 통해 처리된 출력에 드롭아웃을 적용\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # 마지막으로, 이전 출력과 더한 후 다시 레이어 정규화를 수행하여 최종 출력을 생성\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4280dcf6-9f90-4216-aaa8-e2d945b24a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer): # 트랜스포머 모델의 입력으로 사용될 토큰 및 위치 임베딩을 생성하는 레이어를 정의\n",
    "                                        # 입력 시퀀스의 각 토큰에 대한 임베딩과 해당 위치에 대한 임베딩을 결합하여 최종 임베딩을 생성\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.maxlen = maxlen # maxlen은 입력 시퀀스의 최대 길이\n",
    "        self.embed_dim = embed_dim # embed_dim은 토큰 및 위치 임베딩의 차원\n",
    "        self.token_emb = Dense(embed_dim) # token_emb는 입력 토큰에 대한 임베딩을 생성하는 밀집 레이어\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim) # pos_emb는 입력 시퀀스의 각 위치에 대한 임베딩을 생성하는 임베딩 레이어\n",
    "                                                                         # 위치 임베딩의 크기는 (maxlen, embed_dim)\n",
    "\n",
    "    def call(self, x): # 입력으로 주어진 토큰 시퀀스에 대해 위치 임베딩을 생성\n",
    "        maxlen = tf.shape(x)[-2] \n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # 각 위치에 대한 임베딩은 Embedding 레이어를 사용하여 생성되며, 시퀀스의 각 위치에 해당하는 임베딩을 가져와서 positions 변수에 저장\n",
    "        positions = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(x) # 입력 토큰에 대한 임베딩은 token_emb 레이어를 사용하여 생성\n",
    "        return token_embeddings + positions # 위치 임베딩과 토큰 임베딩을 더하여 최종 임베딩을 생성하고 반환\n",
    "\n",
    "# 문장을 한번에 병렬 처리해버리는 Transformer에 단어 순서를 알려주기 위한 작업\n",
    "# 이 모델을 사용하면 입력 시퀀스의 각 토큰에 대해 임베딩을 생성하고, 해당 토큰의 위치 정보를 포함하여 트랜스포머 모델에 입력으로 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8590842c-125c-4d93-a302-13c2cb969fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(input_shape, output_steps, embed_dim, num_heads, ff_dim, num_transformer_blocks): # 트랜스포머 모델을 구축하는 데 사용\n",
    "    # 모델의 입력 형태, 출력의 스텝 수, 임베딩 차원, 어텐션 헤드 수, 피드 포워드 신경망의 은닉층 크기, 트랜스포머 블록의 수\n",
    "    inputs = Input(shape=input_shape) # 입력 데이터를 받기 위한 Keras의 Input 레이어를 생성\n",
    "    embedding_layer = TokenAndPositionEmbedding(input_shape[0], embed_dim) # TokenAndPositionEmbedding 클래스를 사용하여 입력 데이터에 토큰 및 위치 임베딩을 적용\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(num_transformer_blocks): # 지정된 개수의 트랜스포머 블록을 생성하고 이전 블록의 출력을 현재 블록의 입력으로 전달\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x) # 모든 트랜스포머 블록이 생성된 후, 출력을 하나의 값으로 예측하기 위해 Dense 레이어를 적용\n",
    "    x = Dense(1)(x)  # 수정된 부분: Dense(1)을 사용하여 각 타임 스텝에 대해 하나의 값을 예측\n",
    "    x = tf.keras.layers.Flatten()(x)  # 수정된 부분: 출력을 평탄화하여 맞는 차원으로 변환\n",
    "    outputs = Dense(output_steps)(x)  # 최종 출력 레이어\n",
    "    model = Model(inputs=inputs, outputs=outputs) # 출력을 평탄화하여 차원을 맞추고, 최종 출력 레이어를 추가하여 최종 모델을 생성\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d8fa38-c08b-4c47-a748-48d45fe5a703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\koreait-gpu\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">611</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">21,248</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)               │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,272</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m611\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │          \u001b[38;5;34m21,248\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │           \u001b[38;5;34m6,464\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │           \u001b[38;5;34m6,464\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m1\u001b[0m)               │              \u001b[38;5;34m33\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │           \u001b[38;5;34m1,272\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,481</span> (138.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m35,481\u001b[0m (138.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">35,481</span> (138.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m35,481\u001b[0m (138.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (input_steps, x_train.shape[2])\n",
    "embed_dim = 32 # 입력 임베딩 차원\n",
    "num_heads = 2 # 멀티 헤드 셀프 어텐션에서 사용할 어텐션 헤드의 수\n",
    "ff_dim = 32 # 피드 포워드 신경망의 은닉층 크기\n",
    "num_transformer_blocks = 2\n",
    "\n",
    "model = build_transformer_model(input_shape, output_steps, embed_dim, num_heads, ff_dim, num_transformer_blocks)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./transformer-model.keras\", save_best_only=True)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2e783b-f270-4b7a-9b62-1ffc312ce783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 1790.0867 - val_loss: 1025.4814\n",
      "Epoch 2/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 887.3341 - val_loss: 611.3013\n",
      "Epoch 3/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 610.8051 - val_loss: 511.7403\n",
      "Epoch 4/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 511.4307 - val_loss: 462.1253\n",
      "Epoch 5/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 456.2718 - val_loss: 426.9697\n",
      "Epoch 6/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 426.3393 - val_loss: 414.1013\n",
      "Epoch 7/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 436.3538 - val_loss: 399.2045\n",
      "Epoch 8/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 416.5264 - val_loss: 396.9168\n",
      "Epoch 9/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 365.3359 - val_loss: 390.4256\n",
      "Epoch 10/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 389.1505 - val_loss: 389.9708\n",
      "Epoch 11/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 383.9828 - val_loss: 380.3276\n",
      "Epoch 12/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 359.2735 - val_loss: 379.2879\n",
      "Epoch 13/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 375.9783 - val_loss: 373.7722\n",
      "Epoch 14/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 392.4529 - val_loss: 374.5309\n",
      "Epoch 15/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 342.8474 - val_loss: 371.6830\n",
      "Epoch 16/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 355.3160 - val_loss: 370.0531\n",
      "Epoch 17/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 357.7275 - val_loss: 367.1376\n",
      "Epoch 18/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 347.4737 - val_loss: 362.2572\n",
      "Epoch 19/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 362.8963 - val_loss: 361.4536\n",
      "Epoch 20/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 355.7312 - val_loss: 351.8747\n",
      "Epoch 21/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 343.4187 - val_loss: 344.7827\n",
      "Epoch 22/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 324.3335 - val_loss: 333.4429\n",
      "Epoch 23/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 339.2905 - val_loss: 323.3842\n",
      "Epoch 24/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 314.7682 - val_loss: 311.5513\n",
      "Epoch 25/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 289.5363 - val_loss: 293.5334\n",
      "Epoch 26/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 267.2104 - val_loss: 276.3215\n",
      "Epoch 27/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 269.4516 - val_loss: 255.4595\n",
      "Epoch 28/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 236.6935 - val_loss: 238.4077\n",
      "Epoch 29/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 217.1451 - val_loss: 228.7579\n",
      "Epoch 30/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 207.5846 - val_loss: 213.3680\n",
      "Epoch 31/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 192.0358 - val_loss: 197.0818\n",
      "Epoch 32/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 177.2493 - val_loss: 182.3214\n",
      "Epoch 33/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 158.0634 - val_loss: 174.1254\n",
      "Epoch 34/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 148.4280 - val_loss: 163.9817\n",
      "Epoch 35/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 127.8587 - val_loss: 157.7942\n",
      "Epoch 36/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 133.6062 - val_loss: 150.1216\n",
      "Epoch 37/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 119.0672 - val_loss: 147.0799\n",
      "Epoch 38/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 114.4860 - val_loss: 136.9655\n",
      "Epoch 39/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 107.9158 - val_loss: 128.2228\n",
      "Epoch 40/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 101.8500 - val_loss: 122.0894\n",
      "Epoch 41/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 96.2980 - val_loss: 118.1980\n",
      "Epoch 42/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 95.8222 - val_loss: 111.9269\n",
      "Epoch 43/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 84.2030 - val_loss: 109.3835\n",
      "Epoch 44/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 84.1456 - val_loss: 106.6849\n",
      "Epoch 45/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 81.5142 - val_loss: 99.9743\n",
      "Epoch 46/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 77.1551 - val_loss: 99.8420\n",
      "Epoch 47/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 75.6631 - val_loss: 94.4940\n",
      "Epoch 48/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 68.3425 - val_loss: 91.6404\n",
      "Epoch 49/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 70.1440 - val_loss: 88.8395\n",
      "Epoch 50/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 63.4822 - val_loss: 86.3239\n",
      "Epoch 51/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 65.3951 - val_loss: 84.8721\n",
      "Epoch 52/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 61.9200 - val_loss: 81.5763\n",
      "Epoch 53/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 60.7143 - val_loss: 82.3192\n",
      "Epoch 54/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 59.6337 - val_loss: 81.9560\n",
      "Epoch 55/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 57.5895 - val_loss: 76.9582\n",
      "Epoch 56/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 55.8798 - val_loss: 74.3796\n",
      "Epoch 57/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 52.0589 - val_loss: 73.1532\n",
      "Epoch 58/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 52.4750 - val_loss: 72.1779\n",
      "Epoch 59/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 51.2198 - val_loss: 72.9697\n",
      "Epoch 60/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 49.9883 - val_loss: 69.2459\n",
      "Epoch 61/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 49.0430 - val_loss: 68.7079\n",
      "Epoch 62/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 49.3180 - val_loss: 70.0321\n",
      "Epoch 63/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 46.9311 - val_loss: 66.8677\n",
      "Epoch 64/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 47.6914 - val_loss: 64.9754\n",
      "Epoch 65/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 45.6145 - val_loss: 60.8412\n",
      "Epoch 66/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 44.9433 - val_loss: 58.3205\n",
      "Epoch 67/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 40.8601 - val_loss: 56.8907\n",
      "Epoch 68/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 40.5361 - val_loss: 55.9631\n",
      "Epoch 69/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 40.5001 - val_loss: 56.9952\n",
      "Epoch 70/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 40.3414 - val_loss: 53.5836\n",
      "Epoch 71/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 38.2225 - val_loss: 53.5895\n",
      "Epoch 72/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 36.4193 - val_loss: 51.9096\n",
      "Epoch 73/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 35.1428 - val_loss: 49.9832\n",
      "Epoch 74/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 34.6857 - val_loss: 49.3726\n",
      "Epoch 75/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 34.7619 - val_loss: 46.7449\n",
      "Epoch 76/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 33.4449 - val_loss: 46.7399\n",
      "Epoch 77/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 31.8733 - val_loss: 46.1958\n",
      "Epoch 78/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 30.7983 - val_loss: 43.2816\n",
      "Epoch 79/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 29.7682 - val_loss: 42.7339\n",
      "Epoch 80/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 29.4779 - val_loss: 41.2775\n",
      "Epoch 81/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 28.3899 - val_loss: 40.3487\n",
      "Epoch 82/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.9880 - val_loss: 40.1838\n",
      "Epoch 83/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.3742 - val_loss: 36.7354\n",
      "Epoch 84/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26.0502 - val_loss: 37.7512\n",
      "Epoch 85/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.4495 - val_loss: 35.5358\n",
      "Epoch 86/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.1326 - val_loss: 36.3226\n",
      "Epoch 87/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.2726 - val_loss: 35.6451\n",
      "Epoch 88/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.0788 - val_loss: 33.3124\n",
      "Epoch 89/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.7352 - val_loss: 34.2998\n",
      "Epoch 90/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.6328 - val_loss: 30.3669\n",
      "Epoch 91/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.7686 - val_loss: 30.4159\n",
      "Epoch 92/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.2274 - val_loss: 30.0010\n",
      "Epoch 93/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.5141 - val_loss: 31.7095\n",
      "Epoch 94/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.0918 - val_loss: 28.8394\n",
      "Epoch 95/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20.3119 - val_loss: 27.1169\n",
      "Epoch 96/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.9556 - val_loss: 27.4963\n",
      "Epoch 97/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 19.8752 - val_loss: 25.8762\n",
      "Epoch 98/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.5934 - val_loss: 26.0934\n",
      "Epoch 99/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.5952 - val_loss: 24.6489\n",
      "Epoch 100/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.0440 - val_loss: 24.2700\n",
      "Epoch 101/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.8819 - val_loss: 25.4174\n",
      "Epoch 102/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.6032 - val_loss: 24.5652\n",
      "Epoch 103/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17.5071 - val_loss: 22.8669\n",
      "Epoch 104/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16.8314 - val_loss: 22.2993\n",
      "Epoch 105/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.7162 - val_loss: 23.1950\n",
      "Epoch 106/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.6223 - val_loss: 22.1717\n",
      "Epoch 107/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.4039 - val_loss: 21.8378\n",
      "Epoch 108/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.6971 - val_loss: 20.9225\n",
      "Epoch 109/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.1970 - val_loss: 20.0544\n",
      "Epoch 110/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.9109 - val_loss: 18.8893\n",
      "Epoch 111/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.8607 - val_loss: 18.7192\n",
      "Epoch 112/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.1619 - val_loss: 19.4802\n",
      "Epoch 113/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.2252 - val_loss: 18.9807\n",
      "Epoch 114/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1963 - val_loss: 17.5117\n",
      "Epoch 115/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.3434 - val_loss: 17.0478\n",
      "Epoch 116/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.5884 - val_loss: 17.4695\n",
      "Epoch 117/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.7511 - val_loss: 16.6979\n",
      "Epoch 118/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.2846 - val_loss: 16.1756\n",
      "Epoch 119/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.9091 - val_loss: 15.7179\n",
      "Epoch 120/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.9074 - val_loss: 16.4272\n",
      "Epoch 121/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.7243 - val_loss: 16.2321\n",
      "Epoch 122/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.8323 - val_loss: 15.4915\n",
      "Epoch 123/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.6788 - val_loss: 17.0382\n",
      "Epoch 124/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.5939 - val_loss: 14.3479\n",
      "Epoch 125/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.9750 - val_loss: 14.6236\n",
      "Epoch 126/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.6364 - val_loss: 15.1427\n",
      "Epoch 127/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.2788 - val_loss: 13.7550\n",
      "Epoch 128/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.6135 - val_loss: 13.5031\n",
      "Epoch 129/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.2616 - val_loss: 13.7614\n",
      "Epoch 130/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.8147 - val_loss: 13.5309\n",
      "Epoch 131/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.3729 - val_loss: 12.9629\n",
      "Epoch 132/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10.3233 - val_loss: 12.7040\n",
      "Epoch 133/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.9451 - val_loss: 12.0783\n",
      "Epoch 134/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.8000 - val_loss: 12.1993\n",
      "Epoch 135/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.3421 - val_loss: 14.1323\n",
      "Epoch 136/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.3653 - val_loss: 12.0745\n",
      "Epoch 137/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.4118 - val_loss: 11.7371\n",
      "Epoch 138/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.0218 - val_loss: 11.8738\n",
      "Epoch 139/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.7794 - val_loss: 12.7387\n",
      "Epoch 140/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5653 - val_loss: 11.1156\n",
      "Epoch 141/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5918 - val_loss: 11.6976\n",
      "Epoch 142/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7848 - val_loss: 10.5455\n",
      "Epoch 143/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7485 - val_loss: 10.3817\n",
      "Epoch 144/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5992 - val_loss: 10.6197\n",
      "Epoch 145/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.8182 - val_loss: 11.4449\n",
      "Epoch 146/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.0030 - val_loss: 13.2903\n",
      "Epoch 147/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5037 - val_loss: 10.1895\n",
      "Epoch 148/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1796 - val_loss: 10.8312\n",
      "Epoch 149/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1599 - val_loss: 10.2600\n",
      "Epoch 150/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0399 - val_loss: 10.4685\n",
      "Epoch 151/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3785 - val_loss: 9.8660\n",
      "Epoch 152/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8845 - val_loss: 9.6370\n",
      "Epoch 153/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7276 - val_loss: 9.3644\n",
      "Epoch 154/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2800 - val_loss: 9.0047\n",
      "Epoch 155/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3950 - val_loss: 9.1595\n",
      "Epoch 156/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3612 - val_loss: 11.1093\n",
      "Epoch 157/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.4276 - val_loss: 9.2177\n",
      "Epoch 158/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1606 - val_loss: 9.6047\n",
      "Epoch 159/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2045 - val_loss: 8.6707\n",
      "Epoch 160/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1443 - val_loss: 8.6233\n",
      "Epoch 161/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1920 - val_loss: 10.2046\n",
      "Epoch 162/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4047 - val_loss: 11.0024\n",
      "Epoch 163/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3216 - val_loss: 8.4834\n",
      "Epoch 164/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8193 - val_loss: 8.9549\n",
      "Epoch 165/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0971 - val_loss: 9.2598\n",
      "Epoch 166/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7957 - val_loss: 8.1638\n",
      "Epoch 167/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4481 - val_loss: 8.5319\n",
      "Epoch 168/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4471 - val_loss: 8.7100\n",
      "Epoch 169/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1164 - val_loss: 9.0043\n",
      "Epoch 170/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7483 - val_loss: 8.6710\n",
      "Epoch 171/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8617 - val_loss: 7.8422\n",
      "Epoch 172/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6946 - val_loss: 8.1941\n",
      "Epoch 173/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.4702 - val_loss: 7.7196\n",
      "Epoch 174/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5808 - val_loss: 7.2771\n",
      "Epoch 175/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2726 - val_loss: 8.0889\n",
      "Epoch 176/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6957 - val_loss: 7.3902\n",
      "Epoch 177/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2483 - val_loss: 7.4806\n",
      "Epoch 178/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1967 - val_loss: 7.2103\n",
      "Epoch 179/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1660 - val_loss: 7.4723\n",
      "Epoch 180/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7245 - val_loss: 7.7748\n",
      "Epoch 181/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4402 - val_loss: 7.0166\n",
      "Epoch 182/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.8617 - val_loss: 6.9619\n",
      "Epoch 183/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1603 - val_loss: 8.4249\n",
      "Epoch 184/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1951 - val_loss: 7.0192\n",
      "Epoch 185/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1478 - val_loss: 7.1539\n",
      "Epoch 186/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.9101 - val_loss: 6.4809\n",
      "Epoch 187/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7097 - val_loss: 7.3607\n",
      "Epoch 188/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2965 - val_loss: 8.0670\n",
      "Epoch 189/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.8766 - val_loss: 6.5946\n",
      "Epoch 190/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5864 - val_loss: 7.2497\n",
      "Epoch 191/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2948 - val_loss: 6.5828\n",
      "Epoch 192/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9861 - val_loss: 6.7327\n",
      "Epoch 193/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5454 - val_loss: 6.5301\n",
      "Epoch 194/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5525 - val_loss: 6.4790\n",
      "Epoch 195/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2723 - val_loss: 6.8743\n",
      "Epoch 196/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1990 - val_loss: 6.5688\n",
      "Epoch 197/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5755 - val_loss: 6.3884\n",
      "Epoch 198/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5658 - val_loss: 6.6859\n",
      "Epoch 199/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2016 - val_loss: 6.0378\n",
      "Epoch 200/200\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3258 - val_loss: 6.1101\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6700 \n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(x_train, y_train, epochs=200, validation_data=(x_val, y_val), callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# 모델 평가 및 예측\n",
    "model.evaluate(x_test, y_test)\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd113d9-f570-4443-9fa1-3dcebc5d6470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 72.17364  ,  69.49634  ,  87.33026  ,  70.4688   ,  68.77903  ,\n",
       "         77.41383  ,  61.996014 ,  55.128338 ,  68.04587  ,  81.88378  ,\n",
       "         93.29213  ,  77.44663  ,  85.68073  , 101.974594 ,  99.31532  ,\n",
       "        108.17283  ,  83.25248  ,  61.168198 ,  50.361984 ,  32.117596 ,\n",
       "         30.6617   ,  32.784607 ,  31.603687 ,  31.568756 ],\n",
       "       [ 20.861454 ,  16.420088 ,  16.21215  ,   9.101044 ,  22.738459 ,\n",
       "         22.133415 ,  38.392967 ,  60.65935  ,  90.98017  , 104.06591  ,\n",
       "        105.86627  ,  76.68277  ,  68.85672  ,  65.20919  ,  60.796932 ,\n",
       "         69.95755  ,  73.76267  ,  76.46447  ,  47.324593 ,  14.040215 ,\n",
       "         22.741623 ,  24.973751 ,  47.435337 ,  99.24314  ],\n",
       "       [ 31.39444  ,  38.05339  ,  32.158607 ,  22.321983 ,  19.875393 ,\n",
       "         25.145008 ,  26.795355 ,  26.117208 ,  26.614246 ,  40.368652 ,\n",
       "         27.895851 ,  23.545353 ,  18.08906  ,  24.201962 ,  30.798943 ,\n",
       "         31.010073 ,  31.738043 ,  29.61192  ,  26.97863  ,  20.451904 ,\n",
       "         14.791899 ,  19.699558 ,  30.139147 ,  21.429993 ],\n",
       "       [ 55.22102  ,  44.768616 ,  41.15656  ,  27.065666 ,  31.914085 ,\n",
       "         38.22366  ,  23.16365  ,  26.133814 ,  44.218994 ,  61.463223 ,\n",
       "         40.645332 ,  34.667164 ,  49.472855 ,  69.17939  ,  63.01268  ,\n",
       "         52.581303 ,  68.32929  ,  74.02602  ,  73.91515  ,  58.306347 ,\n",
       "         60.16642  ,  70.81191  ,  65.890076 ,  66.0754   ],\n",
       "       [ 15.247988 ,   4.4045625,   3.2452338,  13.296259 ,  13.458804 ,\n",
       "         16.35726  ,  11.819622 ,  14.901137 ,  14.654708 ,  13.0034485,\n",
       "         13.90633  ,   9.62873  ,   8.4709   ,   4.3000226,   4.160502 ,\n",
       "          4.9460397,  18.018515 ,  22.350954 ,  21.5483   ,  21.869745 ,\n",
       "         27.918615 ,  24.547962 ,  21.534235 ,  29.62988  ],\n",
       "       [ 56.504482 ,  57.137184 ,  48.721634 ,  41.337128 ,  56.84965  ,\n",
       "         62.46518  ,  60.05784  ,  38.60538  ,  44.469368 ,  42.274136 ,\n",
       "         46.01342  ,  48.6257   ,  55.04561  ,  45.2189   ,  44.706406 ,\n",
       "         27.571228 ,  34.869923 ,  38.59002  ,  24.449541 ,  24.856459 ,\n",
       "         44.89756  ,  57.617897 ,  40.102077 ,  39.360188 ],\n",
       "       [ 21.114683 ,  34.467968 ,  35.260647 ,  55.298645 ,  42.68352  ,\n",
       "         41.151306 ,  30.66665  ,  18.622395 ,  39.363583 ,  13.929569 ,\n",
       "         16.112785 ,  11.644333 ,  20.349028 ,  20.817356 ,  17.775446 ,\n",
       "         19.154408 ,  20.364101 ,  39.741875 ,  34.822678 ,  25.227913 ,\n",
       "         19.59181  ,  29.86392  ,  22.048906 ,  31.160349 ],\n",
       "       [ 57.765068 ,  23.497448 ,  30.27774  ,  38.308147 ,  41.013733 ,\n",
       "         43.57966  ,  33.707188 ,  28.06739  ,  27.05287  ,  17.655659 ,\n",
       "         25.766315 ,  26.380434 ,  32.451645 ,  27.24374  ,  18.242027 ,\n",
       "         35.4122   ,  35.247147 ,  10.499383 ,  24.979408 ,  26.060255 ,\n",
       "         37.231552 ,  51.32082  ,  70.216736 ,  78.903694 ],\n",
       "       [ 11.949123 ,   6.0939584,   4.0035734,   5.4229503,   5.646859 ,\n",
       "          7.2413   ,  15.241752 ,  27.817474 ,  43.435905 ,  29.986952 ,\n",
       "         24.799038 ,  25.081303 ,  29.60789  ,  20.282925 ,  24.20832  ,\n",
       "         16.62615  ,  13.546483 ,  21.568058 ,  18.099575 ,  13.516071 ,\n",
       "         13.449052 ,  12.524219 ,  17.03613  ,  11.61199  ],\n",
       "       [149.5961   ,  85.73431  ,  70.080246 ,  94.63854  , 111.63603  ,\n",
       "         88.2012   ,  60.44032  ,  49.108215 ,  17.152327 ,  21.541891 ,\n",
       "         30.391438 ,  38.254757 ,  55.170338 ,  55.81455  ,  57.68813  ,\n",
       "         42.38001  ,  31.562931 ,  40.873554 ,  45.72597  ,  36.343998 ,\n",
       "         55.977917 ,  46.820354 ,  44.40858  ,  15.841423 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a01192-ab9f-4589-80c2-f61463a5c722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.,  69.,  86.,  70.,  69.,  77.,  60.,  54.,  67.,  80.,  91.,\n",
       "         76.,  85.,  99.,  96., 104.,  80.,  57.,  49.,  32.,  28.,  30.,\n",
       "         28.,  31.],\n",
       "       [ 22.,  15.,  11.,   9.,  23.,  20.,  37.,  57.,  90., 105., 105.,\n",
       "         76.,  71.,  67.,  65.,  73.,  75.,  80.,  49.,  15.,  22.,  28.,\n",
       "         46., 102.],\n",
       "       [ 26.,  38.,  33.,  23.,  19.,  23.,  28.,  28.,  24.,  39.,  27.,\n",
       "         23.,  19.,  24.,  30.,  30.,  32.,  28.,  27.,  20.,  13.,  19.,\n",
       "         30.,  23.],\n",
       "       [ 56.,  45.,  44.,  28.,  32.,  38.,  23.,  25.,  44.,  60.,  39.,\n",
       "         34.,  50.,  69.,  63.,  50.,  65.,  71.,  72.,  57.,  60.,  72.,\n",
       "         65.,  63.],\n",
       "       [ 15.,   9.,   5.,  15.,  14.,  17.,  15.,  19.,  16.,  15.,  13.,\n",
       "          8.,   7.,   5.,   5.,   8.,  19.,  24.,  21.,  22.,  27.,  23.,\n",
       "         24.,  25.],\n",
       "       [ 60.,  57.,  46.,  42.,  57.,  62.,  59.,  37.,  43.,  42.,  46.,\n",
       "         49.,  56.,  45.,  44.,  28.,  32.,  38.,  23.,  25.,  44.,  60.,\n",
       "         39.,  34.],\n",
       "       [ 22.,  36.,  36.,  56.,  43.,  41.,  29.,  19.,  40.,  16.,  17.,\n",
       "         12.,  20.,  22.,  19.,  20.,  20.,  40.,  38.,  28.,  21.,  31.,\n",
       "         23.,  32.],\n",
       "       [ 57.,  23.,  25.,  40.,  43.,  43.,  30.,  26.,  24.,  15.,  26.,\n",
       "         27.,  36.,  33.,  20.,  38.,  34.,  12.,  28.,  27.,  27.,  47.,\n",
       "         66.,  92.],\n",
       "       [  5.,   9.,   4.,   7.,   8.,  10.,  16.,  28.,  40.,  29.,  26.,\n",
       "         26.,  32.,  20.,  23.,  18.,  13.,  21.,  17.,  13.,  13.,  12.,\n",
       "         16.,  14.],\n",
       "       [149.,  79.,  62.,  85., 108.,  82.,  56.,  49.,  15.,  24.,  30.,\n",
       "         37.,  54.,  54.,  49.,  39.,  31.,  38.,  42.,  37.,  50.,  44.,\n",
       "         45.,  22.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f493e69f-a759-4751-8f45-cc62554f5de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 24)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdc6a5-7cbb-4310-ab53-2b6b74a6167e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
